{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit and explicit hateful speech detection by HateBert\n",
    "\n",
    "## 1. Import and initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ef75209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score,classification_report\n",
    "from IPython.display import clear_output\n",
    "from torch.optim import AdamW\n",
    "from datetime import datetime\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'GroNLP/hateBERT'\n",
    "DATA_PATH = 'data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv' \n",
    "RESULTS_PATH = 'results/'\n",
    "\n",
    "\n",
    "MAX_LENGTH = 512 #max size of the tokenizer https://huggingface.co/GroNLP/hateBERT/commit/f56d507e4b6a64413aff29e541e1b2178ee79d67\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.3\n",
    "PATIENCE = 3\n",
    "TEST_SPLIT_SIZE = 0.2 # validation split\n",
    "RANDOM_SEED = 43\n",
    "NUM_LABELS = 3 # 0: not hate, 1: implicit hate, 2: explicit hate /// \n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "RESULTS_FOLDER = RESULTS_PATH + f\"results_{timestamp}/\"\n",
    "METRICES_FOLDER = RESULTS_PATH + \"metrices/\"\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'results/results_2025-05-21_21-44/' created successfully.\n"
     ]
    }
   ],
   "source": [
    "directory_name = RESULTS_FOLDER\n",
    "# Create the directory\n",
    "try:\n",
    "    os.mkdir(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    post          class\n",
      "0        \" : jewish harvard professor noel ignatiev w...  implicit_hate\n",
      "1       b.higher education is a part of european cult...       not_hate\n",
      "2       has a problem with  \" the whites \" \" and \" \" ...       not_hate\n",
      "3       is yasir qadhi a hate preacher for calling ch...       not_hate\n",
      "4       rt  \" : how three million germans mass murder...       not_hate\n",
      "...                                                  ...            ...\n",
      "21475  Seeing prostitutes is morally wrong, but being...  implicit_hate\n",
      "21476  I wonder how many females I raped today I spen...  implicit_hate\n",
      "21477  Having a criminal record is more attractive to...  implicit_hate\n",
      "21478  Another advantage of being a female: getting b...  implicit_hate\n",
      "21479  If men are so privileged and women have it so ...  implicit_hate\n",
      "\n",
      "[21480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_PATH, sep = '\\t')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Data preparation (labels and text extraction and remaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before mapping: \n",
      " [1 0 0 0 0 0 1 0 0 0 0]\n",
      "Labels after mapping:   [1 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Can select only a subset of the data\n",
    "data = data.head(50)\n",
    "\n",
    "# Label mappings\n",
    "id2label = {0: \"not_hate\", 1: \"implicit_hate\"}#, 2: \"explicit_hate\"}\n",
    "label2id = {\"not_hate\": 0, \"implicit_hate\": 1}#, \"explicit_hate\": 2}\n",
    "\n",
    "#Remove the explicit hate speech to do binary classification\n",
    "data = data[data[\"class\"] != \"explicit_hate\"]\n",
    "\n",
    "# Map labels to numeric values\n",
    "data['class'] = data['class'].map(label2id)\n",
    "\n",
    "# Print raw numeric labels\n",
    "print(\"Labels before mapping: \\n\", data['class'].values[:11])\n",
    "\n",
    "# Load data text\n",
    "texts = data['post'].values\n",
    "\n",
    "labels = data['class'].values\n",
    "# Print string labels\n",
    "print(\"Labels after mapping:  \", labels[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Data Set  Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Plot\n",
    "ax = class_counts.plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for i, count in enumerate(class_counts):\n",
    "    plt.text(i, count + max(class_counts)*0.01, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(RESULTS_FOLDER + \"class_distribution.png\")\n",
    "#plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Load Hate Bert model\n",
    "\n",
    "\n",
    "\n",
    " We decide to use the Hate Bert model, a Bert model specially trained to detect hate. This model can be use from hugging face [plateforme](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109483778\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    hidden_dropout_prob=DROPOUT,  # Increase dropout probability\n",
    "    attention_probs_dropout_prob=DROPOUT  # Increase attention dropout\n",
    ")\n",
    "\n",
    "print(model.num_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 7. Load Tokenizer\n",
    "\n",
    "\n",
    "\n",
    " From hugging face plateforme, we can also load the tokenizer specially made for Hate Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 8. Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return  {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 9. Dataset and DataLoader Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To train our model, we will split the data in 3 categories as it is usually recommanded:\n",
    "\n",
    " - *Training*: The actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data\n",
    "\n",
    " - *Validation*: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.\n",
    "\n",
    " - *Testing*: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "\n",
    "\n",
    " [Source](https://medium.com/data-science/train-validation-and-test-sets-72cb40cba9e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data (60% train, 20% validation and 20% test)\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# splitting by 0.25 because: 0.25 x 0.8 = 0.2\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_val_texts, train_val_labels, test_size=0.25, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "# TRAIN dataset\n",
    "train_dataset = HateSpeechDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# VALIDATION dataset\n",
    "val_dataset = HateSpeechDataset(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "# TEST dataset\n",
    "test_dataset = HateSpeechDataset(\n",
    "    texts=test_texts,\n",
    "    labels=test_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "# DATALOADER for training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# DATALOADER for validation set\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# DATALOADER for testing set\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 10. Training Configuration\n",
    "\n",
    "\n",
    "\n",
    " We use the default training configuration from the kaggle page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Class distribution from your dataset\n",
    "class_counts = class_counts\n",
    "total = sum(class_counts)\n",
    "\n",
    "# Inverse frequency (optional: normalize)\n",
    "class_weights = [total / c for c in class_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Use weighted BCEWithLogitsLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 11. Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "# feel free to experiment with different num_warmup_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=1, num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 12. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, metrics, train_dataloader, device, epoch, progress_bar):\n",
    "    # Put the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Initialize epoch loss\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for iterating over the dataloader to see epoch progress\n",
    "    train_iterator = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{EPOCHS} Training', leave=False)\n",
    "\n",
    "    # Iterate over batches in training set\n",
    "    for batch in train_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        # Forward pass, get the outputs from the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Perform one step of the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        if 'lr_scheduler' in globals():\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "         # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "    \n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "\n",
    "    # Clear the output and print epoch statistics\n",
    "    clear_output()  # Clean the prints from previous epochs\n",
    "    print('Train Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, v) for k, v in epoch_metrics.items()]))\n",
    "\n",
    "    return epoch_loss, epoch_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 13. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, metrics, val_dataloader, device, progress_bar):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    eval_iterator = tqdm(val_dataloader, desc='Evaluating Validation Set')\n",
    "\n",
    "    # Iterate over batches of evaluation dataset\n",
    "    for batch in eval_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, get the outputs from the model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "\n",
    "            # Get the logits from the outputs\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, target)\n",
    "    \n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    epoch_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "        \n",
    "    # Average the loss over all batches\n",
    "    epoch_loss /= len(val_dataloader)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print('Eval Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, v) for k, v in epoch_metrics.items()]))\n",
    "\n",
    "    return epoch_loss, epoch_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 14. Plotting the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_loss, val_loss, metrics_names, train_metrics_logs, test_metrics_logs, savePicture = True):\n",
    "    fig, ax = plt.subplots(1, len(metrics_names) + 1, figsize=((len(metrics_names) + 1) * 5, 5))\n",
    "\n",
    "    ax[0].plot(train_loss, c='blue', label='train')\n",
    "    ax[0].plot(val_loss, c='orange', label='validation')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend()\n",
    "\n",
    "    for i in range(len(metrics_names)):\n",
    "        ax[i + 1].plot(train_metrics_logs[i], c='blue', label='train')\n",
    "        ax[i + 1].plot(test_metrics_logs[i], c='orange', label='validation')\n",
    "        ax[i + 1].set_title(metrics_names[i])\n",
    "        ax[i + 1].set_xlabel('epoch')\n",
    "        ax[i + 1].legend()\n",
    "\n",
    "    fig.suptitle(\"Training result of HateBert\")\n",
    "    fig.savefig(RESULTS_FOLDER+ f'training_plot_{timestamp}.png')\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    '''\n",
    "    - metrics_names: the keys/names of the logged metrics\n",
    "    - metrics_log: existing metrics log that will be updated\n",
    "    - new_metrics_dict: epoch_metrics output from train_epoch and evaluate functions\n",
    "    '''\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 15. Iterative training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, optimizer, criterion, metrics, train_loader, val_loader, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    val_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "    num_training_steps = n_epochs * len(train_dataloader)\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps), desc=\"Training Progress\")\n",
    "\n",
    "    print(f\"Starting training for {EPOCHS} epochs...\") # Use EPOCHS from config\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, device,epoch, progress_bar)\n",
    "\n",
    "        test_loss, test_metrics = validation(model, criterion, metrics, val_loader, device, progress_bar)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        val_metrics_log = update_metrics_log(metrics_names, val_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, val_metrics_log)\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"Training completed.\")\n",
    "    return train_metrics_log, val_metrics_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 16 Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(preds, target):\n",
    "    return precision_score(target, preds, average='macro')\n",
    "\n",
    "def recall(preds, target):\n",
    "    return recall_score(target, preds,average='macro')\n",
    "\n",
    "def f1(preds, target):\n",
    "    return f1_score(target, preds, average='macro')\n",
    "\n",
    "def acc(preds, target):\n",
    "    return accuracy_score(target, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 17. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1339,  P: 0.2222, R: 0.2037, ACC: 0.1667, F1-weighted: 0.1439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9bd2c0877246cf96532dd0e32efcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Validation Set:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.3667,  P: 0.0000, R: 0.0000, ACC: 0.0000, F1-weighted: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "metrics = {'P': precision, 'R': recall, 'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "train_metrics_log, test_metrics_log = training_model(model, optimizer, criterion, metrics, train_dataloader, val_dataloader, n_epochs=EPOCHS, device=device)\n",
    "\n",
    "# save model weights\n",
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.mkdir(RESULTS_FOLDER)\n",
    "torch.save(model.state_dict(), RESULTS_FOLDER + f'base_model_{timestamp}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 18. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, metrics, test_dataloader, device, progress_bar):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    test_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    eval_iterator = tqdm(test_dataloader, desc='Evaluating Test Set')\n",
    "\n",
    "    # Iterate over batches of evaluation dataset\n",
    "    for batch in eval_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, get the outputs from the model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "\n",
    "            # Get the logits from the outputs\n",
    "            logits = outputs.logits\n",
    "    \n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "     # Compute metrics on the entire dataset\n",
    "    test_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "    metrics_report = classification_report(all_predictions,all_labels,digits = 3,target_names=[\"not_hate\", \"implicit_hate\", \"explicit_hate\"])\n",
    "        \n",
    "    return test_metrics, metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_process(model, metrics, test_dataloader, device):\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    num_testing_steps = len(test_dataloader)\n",
    "    progress_bar = tqdm(range(num_testing_steps), desc=\"Testing Progress\")\n",
    "\n",
    "    test_metrics, metrics_report = testing(model, metrics, test_dataloader, device, progress_bar)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(\"Training completed.\")\n",
    "    return test_metrics, metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMetrics(metrics, metrics_report):\n",
    "    with open(RESULTS_FOLDER + f\"testing_results_{timestamp}.txt\", \"w\") as f:\n",
    "        f.write(\"Training configuration \\n\")\n",
    "        f.write(f\"Batch size: {BATCH_SIZE} \\n\")\n",
    "        f.write(f\"Epochs: {EPOCHS} \\n\")\n",
    "        f.write(f\"Learning rate: {LEARNING_RATE} \\n\")\n",
    "        f.write(f\"Seed {RANDOM_SEED} \\n\" ) \n",
    "        f.write(f\"Decay {WEIGHT_DECAY} \\n\")\n",
    "        f.write(f\"Dropout: {DROPOUT} \\n  \\n\")\n",
    "        \n",
    "        f.write(\"Testing results metrices \\n \\n\")\n",
    "\n",
    "        for name, score in metrics.items():\n",
    "            f.write(f\"- {name}, : {score} \\n\")\n",
    "\n",
    "        f.write(\"\\n Testing results report \\n \\n\")\n",
    "        f.write(metrics_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMetrics():\n",
    "    with open(RESULTS_FOLDER + f\"testing_results_{timestamp}.txt\") as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931bcce708644388b7c1c0367fcfe9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eca94a9701c4a2783bbb7b1255ab32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_metrics, metrics_report = testing_process(model, metrics, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Save the testing results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59626d-b748-4de2-a2e0-865c83bacef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration \n",
      "Batch size: 2 \n",
      "Epochs: 5 \n",
      "Learning rate: 3e-06 \n",
      "Seed 43 \n",
      "Decay 0.05 \n",
      "Dropout: 0.3 \n",
      "  \n",
      "Testing results metrices \n",
      " \n",
      "- P, : 0.08333333333333333 \n",
      "- R, : 0.3333333333333333 \n",
      "- ACC, : 0.25 \n",
      "- F1-weighted, : 0.13333333333333333 \n",
      "\n",
      " Testing results report \n",
      " \n",
      "Testing results metrices\n"
     ]
    }
   ],
   "source": [
    "saveMetrics(test_metrics, metrics_report)\n",
    "showMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a2bc8f-b441-4a37-ad75-710d722ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(metrices):\n",
    "    data = {\n",
    "    'Name': f\"results_{timestamp}\",\n",
    "    \"Batch size\": BATCH_SIZE,\n",
    "    \"Epochs\": EPOCHS,\n",
    "    \"Learning rate\": LEARNING_RATE,\n",
    "    \"Seed\": RANDOM_SEED,\n",
    "    \"Decay\": WEIGHT_DECAY,\n",
    "    }\n",
    "\n",
    "    data.update(test_metrics)\n",
    "\n",
    "    # Open our existing CSV file in append mode\n",
    "    # Create a file object for this file\n",
    "    with open(RESULTS_PATH + 'results.csv', 'a') as f_object:\n",
    "    \n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        writer_object = writer(f_object)\n",
    "    \n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "        writer_object.writerow(data.values())\n",
    "    \n",
    "        # Close the file object\n",
    "        f_object.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23aa04c4-aeef-46c3-a109-edd7f9663cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveResults(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 19. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc256760-60ef-41ea-8564-a92beff2a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveInference(string):\n",
    "    with open(RESULTS_FOLDER + f\"inference_results_{timestamp}.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed8685e-3fdf-4352-b57d-c73eb7421d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(example_text, example_label, show=False, save=True):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    example_text  : str | Iterable[str]\n",
    "    example_label : int | str | Iterable[int|str]\n",
    "        - Accepts ids or label names; mixed types are fine.\n",
    "    show          : bool   – print each result to stdout\n",
    "    save          : bool   – append all results to the timestamped file\n",
    "\n",
    "    Returns:\n",
    "    List[int]     – model-predicted label ids (same order as input)\n",
    "    \"\"\"\n",
    "    # normalize and wrap\n",
    "    if isinstance(example_text, str):\n",
    "        example_text = [example_text]\n",
    "    if isinstance(example_label, (str, int)):\n",
    "        example_label = [example_label]\n",
    "\n",
    "    if len(example_text) != len(example_label):\n",
    "        raise ValueError(\"Lengths of example_text and example_label must match.\")\n",
    "\n",
    "    # Convert any label names to ids\n",
    "    lbl_ids = [\n",
    "        label2id[lab] if isinstance(lab, str) else int(lab)\n",
    "        for lab in example_label\n",
    "    ]\n",
    "\n",
    "    # tokenise the full batch\n",
    "    encoded_input = tokenizer(\n",
    "        example_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "\n",
    "    #build per example results\n",
    "    results = []\n",
    "    for txt, true_id, pred_id in zip(example_text, lbl_ids, preds):\n",
    "        res = (\n",
    "            f\"Example sentence: {txt}\\n\"\n",
    "            f\" ---- Model classification: {id2label[pred_id]}\\n\"\n",
    "            f\" ---- Real classification:  {id2label[true_id]}\\n\"\n",
    "        )\n",
    "        results.append(res)\n",
    "        if show:\n",
    "            print(res)\n",
    "\n",
    "    # persist + run\n",
    "    if save:\n",
    "        saveInference(\"\\n\".join(results))\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d82d1d6-44f7-4874-803a-bc4f43b61634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I like trains\",\n",
    "    #\"White people should all die\",\n",
    "    \"I love white snow. I would hate black snow though.\",\n",
    "    \"Is it me or there are too many gays in the world?\",\n",
    "    \"I love the new movie with the black guy.\"\n",
    "]\n",
    "true_labels = [\n",
    "    label2id[\"not_hate\"],\n",
    "    #label2id[\"explicit_hate\"],\n",
    "    label2id[\"not_hate\"],\n",
    "    label2id[\"implicit_hate\"],\n",
    "    label2id[\"not_hate\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f3f8515-828f-4a6c-98f7-3566c6f29dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: I like trains\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification:  not_hate\n",
      "\n",
      "Example sentence: I love white snow. I would hate black snow though.\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification:  not_hate\n",
      "\n",
      "Example sentence: Is it me or there are too many gays in the world?\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification:  implicit_hate\n",
      "\n",
      "Example sentence: I love the new movie with the black guy.\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification:  not_hate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classification(sentences, true_labels, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
