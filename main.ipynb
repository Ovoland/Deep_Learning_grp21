{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686ee1da",
   "metadata": {},
   "source": [
    "# DÃ©tection de Discours Haineux Implicite avec HateBERT\n",
    "\n",
    "## 1. Imports et Configuration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef75209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from IPython.display import clear_output\n",
    "from torch.optim import AdamW\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bda22",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d23529",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'GroNLP/hateBERT'\n",
    "DATA_PATH = 'data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv' \n",
    "RESULTS_PATH = 'results/'\n",
    "\n",
    "\n",
    "MAX_LENGTH = 512 #max size of the tokenizer https://huggingface.co/GroNLP/hateBERT/commit/f56d507e4b6a64413aff29e541e1b2178ee79d67\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "TEST_SPLIT_SIZE = 0.2 # validation split\n",
    "RANDOM_SEED = 43\n",
    "NUM_LABELS = 3 # 0: not hate, 1: implicit hate, 2: explicit hate /// \n",
    "K_FOLDS = 5 \n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "RESULTS_FOLDER = RESULTS_PATH + f\"results_{timestamp}/\"\n",
    "METRICES_FOLDER = RESULTS_PATH + \"metrices/\"\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76e4b78-041c-4315-b794-41bc8cb9efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'results/results_2025-05-18_18-32/' created successfully.\n"
     ]
    }
   ],
   "source": [
    "directory_name = RESULTS_FOLDER\n",
    "# Create the directory\n",
    "try:\n",
    "    os.mkdir(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e502c0",
   "metadata": {},
   "source": [
    "## 3. Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3929b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    post          class\n",
      "0        \" : jewish harvard professor noel ignatiev w...  implicit_hate\n",
      "1       b.higher education is a part of european cult...       not_hate\n",
      "2       has a problem with  \" the whites \" \" and \" \" ...       not_hate\n",
      "3       is yasir qadhi a hate preacher for calling ch...       not_hate\n",
      "4       rt  \" : how three million germans mass murder...       not_hate\n",
      "...                                                  ...            ...\n",
      "21475  Seeing prostitutes is morally wrong, but being...  implicit_hate\n",
      "21476  I wonder how many females I raped today I spen...  implicit_hate\n",
      "21477  Having a criminal record is more attractive to...  implicit_hate\n",
      "21478  Another advantage of being a female: getting b...  implicit_hate\n",
      "21479  If men are so privileged and women have it so ...  implicit_hate\n",
      "\n",
      "[21480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_PATH, sep = '\\t')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2721dd",
   "metadata": {},
   "source": [
    "## 4. Data Set  Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Plot\n",
    "ax = class_counts.plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for i, count in enumerate(class_counts):\n",
    "    plt.text(i, count + max(class_counts)*0.01, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(RESULTS_FOLDER + \"class_distribution.png\")\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ff0a7",
   "metadata": {},
   "source": [
    "## 5. Data preparation (labels and text extraction and remaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9470f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before mapping: \n",
      " ['implicit_hate' 'not_hate' 'not_hate' 'not_hate' 'not_hate' 'not_hate'\n",
      " 'implicit_hate' 'not_hate' 'explicit_hate' 'explicit_hate' 'not_hate']\n",
      "Labels after mapping:   [1 0 0 0 0 0 1 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Can select only a subset of the data\n",
    "data = data.head(20)\n",
    "\n",
    "# Label mappings\n",
    "id2label = {0: \"not_hate\", 1: \"implicit_hate\", 2: \"explicit_hate\"}\n",
    "label2id = {\"not_hate\": 0, \"implicit_hate\": 1, \"explicit_hate\": 2}\n",
    "\n",
    "\n",
    "# Load data text\n",
    "texts = data['post'].values\n",
    "\n",
    "# Print raw numeric labels\n",
    "print(\"Labels before mapping: \\n\", data['class'].values[:11])\n",
    "\n",
    "# Map labels to numeric values\n",
    "data['class'] = data['class'].map(label2id)\n",
    "labels = data['class'].values\n",
    "# Print string labels\n",
    "print(\"Labels after mapping:  \", labels[:11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89670793",
   "metadata": {},
   "source": [
    "# 6. Load Hate Bert model\n",
    "\n",
    "We decide to use the Hate Bert model, a Bert model specially trained to detect hate. This model can be use from hugging face [plateforme](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c72488e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49adb75d0414240b1c5100e8e14aae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d28635d2ca4618a977543096e23c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109484547\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5b315",
   "metadata": {},
   "source": [
    "# 7. Load Tokenizer\n",
    "\n",
    "From hugging face plateforme, we can also load the tokenizer specially made for Hate Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae2699a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3760062ec5de4fa4baabc4ae869160a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a586c903722d4642982cd5ac2cd80a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd684aa46a44ebb8cbc6adba2893bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35591758",
   "metadata": {},
   "source": [
    "# 8. Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fea2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return  {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f5141",
   "metadata": {},
   "source": [
    "# 9. Dataset and DataLoader Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f71034",
   "metadata": {},
   "source": [
    "To train our model, we will split the data in 3 categories as it is usually recommanded:\n",
    "- *Training*: The actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data\n",
    "- *Validation*: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. \n",
    "- *Testing*: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "[Source](https://medium.com/data-science/train-validation-and-test-sets-72cb40cba9e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ad20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data (60% train, 20% validation and 20% test)\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# splitting by 0.25 because: 0.25 x 0.8 = 0.2\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.25, random_state=RANDOM_SEED\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "train_index = []\n",
    "val_index = []\n",
    "train_labels = []\n",
    "val_labels = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    train_texts = train_val_texts[train_index]\n",
    "    val_index = train_val_texts[val_index]\n",
    "    train_labels = train_val_labels[train_index]\n",
    "    val_labels = train_val_labels[val_index]\n",
    "    \n",
    "\n",
    "# TRAIN dataset\n",
    "train_dataset = HateSpeechDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# VALIDATION dataset\n",
    "val_dataset = HateSpeechDataset(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATALOADER for training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# DATALOADER for validation set\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# DATALOADER for testing set\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90928ba3",
   "metadata": {},
   "source": [
    "# 10. Training Configuration\n",
    "\n",
    "We use the default training configuration from the kaggle page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0de70d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Class distribution from your dataset\n",
    "class_counts = class_counts\n",
    "total = sum(class_counts)\n",
    "\n",
    "# Inverse frequency (optional: normalize)\n",
    "class_weights = [total / c for c in class_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Use weighted BCEWithLogitsLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ac591",
   "metadata": {},
   "source": [
    "# 11. Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e46a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "# feel free to experiment with different num_warmup_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=1, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa867e6",
   "metadata": {},
   "source": [
    "# 12. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b60603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, metrics, train_dataloader, device, epoch, progress_bar):\n",
    "    # Put the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Initialize epoch loss\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for iterating over the dataloader to see epoch progress\n",
    "    train_iterator = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{EPOCHS} Training', leave=False)\n",
    "\n",
    "    # Iterate over batches in training set\n",
    "    for batch in train_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        # Forward pass, get the outputs from the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform one step of the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        if 'lr_scheduler' in globals():\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "         # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "\n",
    "    # Clear the output and print epoch statistics\n",
    "    clear_output()  # Clean the prints from previous epochs\n",
    "    print('Train Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, v) for k, v in epoch_metrics.items()]))\n",
    "\n",
    "    return epoch_loss, epoch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e81249",
   "metadata": {},
   "source": [
    "# 13. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0688f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, metrics, val_dataloader, device, progress_bar):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    eval_iterator = tqdm(val_dataloader, desc='Evaluating Validation Set')\n",
    "\n",
    "    # Iterate over batches of evaluation dataset\n",
    "    for batch in eval_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, get the outputs from the model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "\n",
    "            # Get the logits from the outputs\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, target)\n",
    "    \n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    epoch_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "        \n",
    "    # Average the loss over all batches\n",
    "    epoch_loss /= len(val_dataloader)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print('Eval Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, v) for k, v in epoch_metrics.items()]))\n",
    "\n",
    "    return epoch_loss, epoch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21db3df",
   "metadata": {},
   "source": [
    "# 14. Plotting the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c191d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_loss, val_loss, metrics_names, train_metrics_logs, test_metrics_logs, savePicture = True):\n",
    "    fig, ax = plt.subplots(1, len(metrics_names) + 1, figsize=((len(metrics_names) + 1) * 5, 5))\n",
    "\n",
    "    ax[0].plot(train_loss, c='blue', label='train')\n",
    "    ax[0].plot(val_loss, c='orange', label='validation')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend()\n",
    "\n",
    "    for i in range(len(metrics_names)):\n",
    "        ax[i + 1].plot(train_metrics_logs[i], c='blue', label='train')\n",
    "        ax[i + 1].plot(test_metrics_logs[i], c='orange', label='validation')\n",
    "        ax[i + 1].set_title(metrics_names[i])\n",
    "        ax[i + 1].set_xlabel('epoch')\n",
    "        ax[i + 1].legend()\n",
    "\n",
    "    fig.suptitle(\"Training result of HateBert\")\n",
    "    fig.savefig(RESULTS_FOLDER+ f'training_plot_{timestamp}.png')\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    '''\n",
    "    - metrics_names: the keys/names of the logged metrics\n",
    "    - metrics_log: existing metrics log that will be updated\n",
    "    - new_metrics_dict: epoch_metrics output from train_epoch and evaluate functions\n",
    "    '''\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28145930",
   "metadata": {},
   "source": [
    "# 15. Iterative training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ae200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, optimizer, criterion, metrics, train_loader, val_loader, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    val_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "    num_training_steps = n_epochs * len(train_dataloader)\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps), desc=\"Training Progress\")\n",
    "\n",
    "    print(f\"Starting training for {EPOCHS} epochs...\") # Use EPOCHS from config\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, device,epoch, progress_bar)\n",
    "\n",
    "        test_loss, test_metrics = validation(model, criterion, metrics, val_loader, device, progress_bar)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        val_metrics_log = update_metrics_log(metrics_names, val_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, val_metrics_log)\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"Training completed.\")\n",
    "    return train_metrics_log, val_metrics_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a84043-488f-473f-ae98-facf7ef38de0",
   "metadata": {},
   "source": [
    "# 16 Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3fc1b7-3576-43c3-ace5-1f1829e7f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(preds, target):\n",
    "    return precision_score(target, preds, average='macro')\n",
    "\n",
    "def recall(preds, target):\n",
    "    return recall_score(target, preds,average='macro')\n",
    "\n",
    "def f1(preds, target):\n",
    "    return f1_score(target, preds, average='macro')\n",
    "\n",
    "def acc(preds, target):\n",
    "    return accuracy_score(target, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba475f0",
   "metadata": {},
   "source": [
    "# 17. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8930925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9914,  P: 0.1667, R: 0.6667, ACC: 0.2500, F1-weighted: 0.2667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2711e3f6034242bb9ff0cdcc76a471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Validation Set:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.3489,  P: 0.0000, R: 0.0000, ACC: 0.0000, F1-weighted: 0.0000\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "metrics = {'P': precision, 'R': recall, 'ACC': acc, 'F1-weighted': f1}\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "train_metrics_log, test_metrics_log = training_model(model, optimizer, criterion, metrics, train_dataloader, val_dataloader, n_epochs=EPOCHS, device=device)\n",
    "\n",
    "# save model weights\n",
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.mkdir(RESULTS_FOLDER)\n",
    "torch.save(model.state_dict(), RESULTS_FOLDER + f'base_model_{timestamp}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a55e3",
   "metadata": {},
   "source": [
    "# 18. Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04d5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, metrics, test_dataloader, device, progress_bar):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    eval_iterator = tqdm(test_dataloader, desc='Evaluating Test Set')\n",
    "\n",
    "    # Iterate over batches of evaluation dataset\n",
    "    for batch in eval_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, get the outputs from the model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "\n",
    "            # Get the logits from the outputs\n",
    "            logits = outputs.logits\n",
    "    \n",
    "        # Use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Compute metrics on the entire dataset\n",
    "    epoch_metrics = {k: metrics[k](all_predictions, all_labels) for k in metrics.keys()}\n",
    "        \n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bed9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_process(model, metrics, test_dataloader, device):\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    num_testing_steps = len(test_dataloader)\n",
    "    progress_bar = tqdm(range(num_testing_steps), desc=\"Testing Progress\")\n",
    "\n",
    "    test_metrics = testing(model, metrics, test_dataloader, device, progress_bar)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(\"Training completed.\")\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd1f19e7-f671-4b85-9a67-a893a6c032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMetrics(metrics, title):\n",
    "    with open(RESULTS_FOLDER + f\"testing_results_{timestamp}.txt\", \"w\") as f:\n",
    "        f.write(\"Training configuration \\n\")\n",
    "        f.write(f\"Batch size: {BATCH_SIZE} \\n\")\n",
    "        f.write(f\"Epochs: {EPOCHS} \\n\")\n",
    "        f.write(f\"Learning rate: {LEARNING_RATE} \\n\")\n",
    "        f.write(f\"Seed {RANDOM_SEED} \\n \\n\") \n",
    "        f.write(f\"{title} \\n\")\n",
    "        for name, score in metrics.items():\n",
    "            f.write(f\"- {name}, : {score} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f6f9380-2547-4be3-ad95-f759c1250ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMetrics():\n",
    "    with open(RESULTS_FOLDER + f\"testing_results_{timestamp}.txt\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "088f49f5-270d-40df-b0ba-000508b3427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817d94829be49698a3380d942660614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c2bb057a849a0b8cf0fb912c109e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jlab-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_metrics = testing_process(model, metrics, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabcc1ac-b380-4ec2-a47d-e5fc86294c08",
   "metadata": {},
   "source": [
    "Save the testing results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba59626d-b748-4de2-a2e0-865c83bacef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration \n",
      "Batch size: 16 \n",
      "Epochs: 5 \n",
      "Learning rate: 2e-05 \n",
      "Seed 43 \n",
      " \n",
      "Testing results metrices \n",
      "- P, : 0.08333333333333333 \n",
      "- R, : 0.3333333333333333 \n",
      "- ACC, : 0.25 \n",
      "- F1-weighted, : 0.13333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "saveMetrics(test_metrics, \"Testing results metrices\")\n",
    "showMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18a2bc8f-b441-4a37-ad75-710d722ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(metrices):\n",
    "    data = {\n",
    "    'Name': f\"results_{timestamp}\",\n",
    "    \"Batch size\": BATCH_SIZE,\n",
    "    \"Epochs\": EPOCHS,\n",
    "    \"Learning rate\": LEARNING_RATE,\n",
    "    \"Seed\": RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    data.update(test_metrics)\n",
    "\n",
    "    # Open our existing CSV file in append mode\n",
    "    # Create a file object for this file\n",
    "    with open(RESULTS_PATH + 'results.csv', 'a') as f_object:\n",
    "    \n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        writer_object = writer(f_object)\n",
    "    \n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "        writer_object.writerow(data.values())\n",
    "    \n",
    "        # Close the file object\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23aa04c4-aeef-46c3-a109-edd7f9663cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveResults(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4d3f5-a21c-49a5-9265-25c76555a118",
   "metadata": {},
   "source": [
    "# 19. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc256760-60ef-41ea-8564-a92beff2a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveInference(string):\n",
    "    with open(RESULTS_FOLDER + f\"inference_results_{timestamp}.txt\", \"w\") as f:\n",
    "      f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ed8685e-3fdf-4352-b57d-c73eb7421d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(example_text, example_label, show = False, save= True): \n",
    "    #Using the HateBertDataLoader is a bit overkilled, we can just tokenize the input\n",
    "    encoded_input = tokenizer(\n",
    "        example_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass, get the outputs from the model\n",
    "        outputs = model(**encoded_input)\n",
    "        \n",
    "        # Get the logits from the outputs\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Use argmax to get the predicted class\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    res_str =  f\"Example sentence: {example_text}\\n ---- Model classification: {id2label[int(preds)]}\\n ---- Real classification: {id2label[example_label]}\\n\"\n",
    "    if show: print(res_str)\n",
    "    if save: saveInference(res_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d82d1d6-44f7-4874-803a-bc4f43b61634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: I like train\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification: not_hate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write here an example sentence\n",
    "example_text = \"I like train\"\n",
    "#Determine the type of hate of your sentence \n",
    "example_label = \"not_hate\" \n",
    "classification(example_text, label2id[example_label], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3f8515-828f-4a6c-98f7-3566c6f29dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: White people should all die\n",
      " ---- Model classification: implicit_hate\n",
      " ---- Real classification: explicit_hate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write here an example sentence\n",
    "example_text = \"White people should all die\"\n",
    "#Determine the type of hate of your sentence \n",
    "example_label = \"explicit_hate\" \n",
    "classification(example_text, label2id[example_label], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
