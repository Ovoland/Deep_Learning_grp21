{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686ee1da",
   "metadata": {},
   "source": [
    "# Détection de Discours Haineux Implicite avec HateBERT\n",
    "\n",
    "## 1. Imports et Configuration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef75209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import evaluate \n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bda22",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23529",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'GroNLP/hateBERT'\n",
    "DATA_PATH = 'data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv' \n",
    "MAX_LENGTH = 512 #max size of the tokenizer https://huggingface.co/GroNLP/hateBERT/commit/f56d507e4b6a64413aff29e541e1b2178ee79d67\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "TEST_SPLIT_SIZE = 0.2 # validation split\n",
    "RANDOM_SEED = 42\n",
    "NUM_LABELS = 3 # 0: not hate, 1: implicit hate, 2: explicit hate /// \n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e502c0",
   "metadata": {},
   "source": [
    "## 3. Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3929b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    post          class\n",
      "0        \" : jewish harvard professor noel ignatiev w...  implicit_hate\n",
      "1       b.higher education is a part of european cult...       not_hate\n",
      "2       has a problem with  \" the whites \" \" and \" \" ...       not_hate\n",
      "3       is yasir qadhi a hate preacher for calling ch...       not_hate\n",
      "4       rt  \" : how three million germans mass murder...       not_hate\n",
      "...                                                  ...            ...\n",
      "21475  Seeing prostitutes is morally wrong, but being...  implicit_hate\n",
      "21476  I wonder how many females I raped today I spen...  implicit_hate\n",
      "21477  Having a criminal record is more attractive to...  implicit_hate\n",
      "21478  Another advantage of being a female: getting b...  implicit_hate\n",
      "21479  If men are so privileged and women have it so ...  implicit_hate\n",
      "\n",
      "[21480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#PATH_DATA = \"data/implicit-hate-corpus/\"\n",
    "#PATH_INPUT_POSTS = \"implicit_hate_v1_SAP_posts.tsv\"\n",
    "#PATH_HATE_LABEL = \"implicit_hate_v1_stg1.tsv\"\n",
    "#PATH_CATEGORY_LABEL = \"implicit_hate_v1_stg2.tsv\"\n",
    "\n",
    "#input = pd.read_csv(PATH_DATA + PATH_INPUT_POSTS, sep = '\\t')\n",
    "#hate_labels = pd.read_csv(PATH_DATA + PATH_HATE_LABEL, sep = '\\t')\n",
    "#category_labels = pd.read_csv(PATH_DATA + PATH_CATEGORY_LABEL, sep = '\\t')\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(DATA_PATH, sep = '\\t')\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2721dd",
   "metadata": {},
   "source": [
    "## 4. Data Set  Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd3c387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class_counts = data['class'].value_counts()\n",
    "\n",
    "# Plot\n",
    "ax = class_counts.plot(kind='bar', title='Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for i, count in enumerate(class_counts):\n",
    "    plt.text(i, count + max(class_counts)*0.01, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ff0a7",
   "metadata": {},
   "source": [
    "## 5. Data preparation (labels and text extraction and remaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9470f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before mapping: \n",
      " ['implicit_hate' 'not_hate' 'not_hate' 'not_hate' 'not_hate' 'not_hate'\n",
      " 'implicit_hate' 'not_hate' 'explicit_hate' 'explicit_hate' 'not_hate']\n",
      "Labels after mapping:   [1 0 0 0 0 0 1 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Label mappings\n",
    "id2label = {0: \"not_hate\", 1: \"implicit_hate\", 2: \"explicit_hate\"}\n",
    "label2id = {\"not_hate\": 0, \"implicit_hate\": 1, \"explicit_hate\": 2}\n",
    "\n",
    "\n",
    "# Load data text\n",
    "texts = data['post'].values\n",
    "\n",
    "# Print raw numeric labels\n",
    "print(\"Labels before mapping: \\n\", data['class'].values[:11])\n",
    "\n",
    "# Map labels to numeric values\n",
    "data['class'] = data['class'].map(label2id)\n",
    "labels = data['class'].values\n",
    "# Print string labels\n",
    "print(\"Labels after mapping:  \", labels[:11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89670793",
   "metadata": {},
   "source": [
    "# 6. Load Hate Bert model\n",
    "\n",
    "We decide to use the Hate Bert model, a Bert model specially trained to detect hate. This model can be use from hugging face [plateforme](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c72488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109484547\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5b315",
   "metadata": {},
   "source": [
    "# 7. Load Tokenizer\n",
    "\n",
    "From hugging face plateforme, we can also load the tokenizer specially made for Hate Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2699a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35591758",
   "metadata": {},
   "source": [
    "# 8. Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fea2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return  {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f5141",
   "metadata": {},
   "source": [
    "# 9. Dataset and DataLoader Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0ad20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data (80% train and 20% test)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# TRAIN dataset\n",
    "train_dataset = HateSpeechDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# TESTING dataset\n",
    "test_dataset = HateSpeechDataset(\n",
    "    texts=test_texts,\n",
    "    labels=test_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "# DATALOADER for training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# DATALOADER for testing set\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90928ba3",
   "metadata": {},
   "source": [
    "# 10. Training Configuration\n",
    "\n",
    "We use the default training configuration from the kaggle page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de70d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Class distribution from your dataset\n",
    "class_counts = [13291, 7100, 1089]\n",
    "total = sum(class_counts)\n",
    "\n",
    "# Inverse frequency (optional: normalize)\n",
    "class_weights = [total / c for c in class_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Use weighted BCEWithLogitsLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ac591",
   "metadata": {},
   "source": [
    "# 11. Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "# feel free to experiment with different num_warmup_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=1, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa867e6",
   "metadata": {},
   "source": [
    "# 12. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b60603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 9/68736 [25:57:23<198212:36:33, 10382.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-course/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Training Progress:   0%|          | 9/3222 [01:35<9:21:21, 10.48s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl-course/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl-course/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl-course/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, optimizer, criterion, metrics, train_dataloader, device, epoch, progress_bar):\n",
    "    \n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # initialize epoch loss and metrics\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for iterating over the dataloader to see epoch progress\n",
    "    train_iterator = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{EPOCHS} Training', leave=False)\n",
    "\n",
    "    # iterate over batches in training set\n",
    "    for batch in train_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        # forward pass, get the outputs from the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "        # get the loss from the outputs\n",
    "        loss = criterion(outputs.logits, target)\n",
    "\n",
    "        # do the backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # perform one step of the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        if 'lr_scheduler' in globals():\n",
    "             lr_scheduler.step()\n",
    "\n",
    "        # zero the gradients, call zero_grad() on the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "           preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "   \n",
    "        with torch.no_grad():\n",
    "            for k in epoch_metrics.keys():\n",
    "                epoch_metrics[k] += metrics[k](preds.cpu(), target.cpu())\n",
    "\n",
    "        # log loss statistics\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # for the epoch loss, we take the average of the losses computed over the mini-batches\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "\n",
    "    # for the epoch loss, we compute the average of the metrics over the mini-batches\n",
    "    for k in epoch_metrics.keys():\n",
    "          epoch_metrics[k] /= len(train_dataloader)\n",
    "\n",
    "    clear_output() #clean the prints from previous epochs\n",
    "    print('train Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, epoch_metrics[k]) for k in epoch_metrics.keys()]))\n",
    "\n",
    "    return epoch_loss,  epoch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e81249",
   "metadata": {},
   "source": [
    "# 13. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set: 100%|██████████| 269/269 [07:48<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3479981378026071}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model, criterion, metrics, test_dataloader, device, epoch, progress_bar):\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_metrics = dict(zip(metrics.keys(), torch.zeros(len(metrics))))\n",
    "\n",
    "    # Use tqdm for the evaluation dataloader\n",
    "    eval_iterator = tqdm(test_dataloader, desc='Evaluating Test Set')\n",
    "\n",
    "    # iterate over batches of evaluation dataset\n",
    "    for batch in eval_iterator:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target = batch[\"labels\"]  # Get the target labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # forward pass, get the outputs from the model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target)\n",
    "\n",
    "        #get the logits from the outputs\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, target)\n",
    "    \n",
    "        # use argmax to get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Append predictions and true labels to the lists\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        for k in epoch_metrics.keys():\n",
    "            epoch_metrics[k] += metrics[k](preds.cpu(), target.cpu())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    epoch_loss /= len(test_dataloader)\n",
    "    \n",
    "    for k in epoch_metrics.keys():\n",
    "          epoch_metrics[k] /= len(test_dataloader)\n",
    "\n",
    "    print('eval Loss: {:.4f}, '.format(epoch_loss),\n",
    "          ', '.join(['{}: {:.4f}'.format(k, epoch_metrics[k]) for k in epoch_metrics.keys()]))\n",
    "    return epoch_loss,  epoch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21db3df",
   "metadata": {},
   "source": [
    "# 14. Plotting the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_loss, test_loss, metrics_names, train_metrics_logs, test_metrics_logs):\n",
    "    fig, ax = plt.subplots(1, len(metrics_names) + 1, figsize=((len(metrics_names) + 1) * 5, 5))\n",
    "\n",
    "    ax[0].plot(train_loss, c='blue', label='train')\n",
    "    ax[0].plot(test_loss, c='orange', label='test')\n",
    "    ax[0].set_title('Loss')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend()\n",
    "\n",
    "    for i in range(len(metrics_names)):\n",
    "        ax[i + 1].plot(train_metrics_logs[i], c='blue', label='train')\n",
    "        ax[i + 1].plot(test_metrics_logs[i], c='orange', label='test')\n",
    "        ax[i + 1].set_title(metrics_names[i])\n",
    "        ax[i + 1].set_xlabel('epoch')\n",
    "        ax[i + 1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def update_metrics_log(metrics_names, metrics_log, new_metrics_dict):\n",
    "    '''\n",
    "    - metrics_names: the keys/names of the logged metrics\n",
    "    - metrics_log: existing metrics log that will be updated\n",
    "    - new_metrics_dict: epoch_metrics output from train_epoch and evaluate functions\n",
    "    '''\n",
    "    for i in range(len(metrics_names)):\n",
    "        curr_metric_name = metrics_names[i]\n",
    "        metrics_log[i].append(new_metrics_dict[curr_metric_name])\n",
    "    return metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28145930",
   "metadata": {},
   "source": [
    "# 15. Iterative training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, optimizer, criterion, metrics, train_loader, test_loader, n_epochs, device):\n",
    "    train_loss_log,  test_loss_log = [], []\n",
    "    metrics_names = list(metrics.keys())\n",
    "    train_metrics_log = [[] for i in range(len(metrics))]\n",
    "    test_metrics_log = [[] for i in range(len(metrics))]\n",
    "\n",
    "    num_training_steps = n_epochs * len(train_dataloader)\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps), desc=\"Training Progress\")\n",
    "\n",
    "    print(f\"Starting training for {EPOCHS} epochs...\") # Use EPOCHS from config\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, criterion, metrics, train_loader, device,epoch, progress_bar)\n",
    "\n",
    "        test_loss, test_metrics = evaluate(model, criterion, metrics, test_loader, device)\n",
    "\n",
    "        train_loss_log.append(train_loss)\n",
    "        train_metrics_log = update_metrics_log(metrics_names, train_metrics_log, train_metrics)\n",
    "\n",
    "        test_loss_log.append(test_loss)\n",
    "        test_metrics_log = update_metrics_log(metrics_names, test_metrics_log, test_metrics)\n",
    "\n",
    "        plot_training(train_loss_log, test_loss_log, metrics_names, train_metrics_log, test_metrics_log)\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"Training completed.\")\n",
    "    return train_metrics_log, test_metrics_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba475f0",
   "metadata": {},
   "source": [
    "# 16. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'ACC': evaluate.load(\"accuracy\"), 'F1-weighted': evaluate.load(\"f1\", average=\"weighted\")}\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "train_metrics_log, test_metrics_log = training_model(model, optimizer, criterion, metrics, train_dataloader, test_dataloader, n_epochs=EPOCHS, device=device)\n",
    "\n",
    "# save model weights\n",
    "results_models_weights_dir = 'models_weights/'\n",
    "if not os.path.exists(results_models_weights_dir):\n",
    "    os.mkdir(results_models_weights_dir)\n",
    "torch.save(model.state_dict(), results_models_weights_dir + 'base_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
